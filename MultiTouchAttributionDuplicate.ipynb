{"cells":[{"cell_type":"code","source":["%run /Users/production@aarp.com/Utils/env_variables"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%run /Users/production@aarp.com/Utils/Utils"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["tempdir = 's3a://aarp-testing/tmp/redshift_copy'"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Facts about this analysis\n* ### 10,000,000,000+ rows of data processed\n* ### Spark cluster with 12 r4.2xlarge instances; 732 GB of RAM, 96 CPUs"],"metadata":{}},{"cell_type":"markdown","source":["## Sidenote:\n\n* ### I like flags. \n* ### I also like window functions. \n* ### I don't like serial processing. Bottlenecks kill me. Parallelize it all"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import udf, lag, when\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import functions as func"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["rs_url = 'jdbc:redshift://aarp-rs-temp.c0vmann988mu.us-east-1.redshift.amazonaws.com:5439/dev?user={}&password={}'.format(rs_user, rs_password)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["# 1. Pull Data\n\n### Import the Data from Redshift"],"metadata":{}},{"cell_type":"markdown","source":["#### Set conversion event types and start time"],"metadata":{}},{"cell_type":"markdown","source":["#### Pull all engagement events"],"metadata":{}},{"cell_type":"markdown","source":["#### Pull distinct engagement events\n##### We pull distinct conversion events because we want to remove the data if multiple conversion of the same type, happen with the same user on the same day. This removes a paths to conversion that are too short and will impact our analysis"],"metadata":{}},{"cell_type":"markdown","source":["#### Pull individual dimension data"],"metadata":{}},{"cell_type":"markdown","source":["#### Union conversion and engagement data to have all event data"],"metadata":{}},{"cell_type":"markdown","source":["# 2. Create Flags\n\n#### Create the conversion flag, to distingush conversion events"],"metadata":{}},{"cell_type":"markdown","source":["### We need to identify when there is a new user as we do not want user paths to get mixed\n#### Order by the individual_key and the date, then create a column that has the previous user id, call it the last_individual_key column"],"metadata":{}},{"cell_type":"code","source":["# Window by users and dates\nlast_user_partition = Window.partitionBy().orderBy('individual_key', 'date_key')\n\n#  Apply Window object to events dataframe to get last individual key\nevents_df = events_df.withColumn('last_individual_key', func.lag(events_df.individual_key).over(last_user_partition))\nevents_df.cache()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### Use the last_individual_key column to identify a new user\n#### last_individual_key column is a 1 if its a new user, 0 if its the same user"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, expr, when\n\nnew_user_flag = expr( \"\"\"IF(individual_key = last_individual_key or last_individual_key is null, 0, 1)\"\"\")\nevents_df = events_df.withColumn('new_user_flag', new_user_flag)\nevents_df.cache()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Use new user flag and conversion flag to mark paths to conversion.\n\n\n#### Create a column that is the increments every time there is a new user or a conversion. This column is then a marker for new conversion paths"],"metadata":{}},{"cell_type":"code","source":["#create partioning window\nindiv_date_key_partition = Window.partitionBy().orderBy('individual_key', 'date_key')\n\n#apply sum function over individual_key and date\nevents_df = events_df.withColumn('conversion_path', func.sum(events_df.conversion_flag + events_df.new_user_flag).over(indiv_date_key_partition))\nevents_df.cache()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Remove last conversion from path\n#### Since the user has not converted since their last conversion flag, we want to flag the last conversion path for a user"],"metadata":{}},{"cell_type":"code","source":["conversion_rank_window = Window.partitionBy('individual_key').orderBy('individual_key', 'conversion_path')\n\nevents_df = events_df.withColumn('conversion_rank', func.rank().over(conversion_rank_window))"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Flag not converters\n\n#### Create a dataframe of distinct individual keys from the converter dataframe."],"metadata":{}},{"cell_type":"code","source":["converter_df = conversion_events_df.select('individual_key').groupby('individual_key').agg({'*':'count'})\n"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["converter_df = converter_df.select(col('individual_key').alias('converter'), col('count(1)').alias('num_conversions'))\n"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["#### Left outer join converter keys to events dataframe, this flags keys as converters or non-converters"],"metadata":{}},{"cell_type":"code","source":["events_df = events_df.join(converter_df, events_df.individual_key == converter_df.converter, 'left_outer')"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["## Create column to be used for model\n#### if the person has never converted, or it is their last path to conversion it is a 0, otherwise its a 1"],"metadata":{}},{"cell_type":"code","source":["model_column = when(col('converter').isNull(), 0).when(col('conversion_rank') == col('num_conversions'), 0).otherwise(1)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["#### Add model column flag to the dataframe"],"metadata":{}},{"cell_type":"code","source":["events_df = events_df.withColumn('successful_conversion', model_column)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["sample_events_df = events_df.sample(True, .0001)\nsample_events_df.createOrReplaceTempView('events_temp_table')"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["# 3. Group dataframe around path\n\n#### Now that we have a key for conversion path, we can group engagements by that flag to give us our data"],"metadata":{}},{"cell_type":"markdown","source":["#### Widen the data with some one hot encoding"],"metadata":{}},{"cell_type":"markdown","source":["### Group by conversion path and individual"],"metadata":{}},{"cell_type":"markdown","source":["### Aggregate all touch points"],"metadata":{}},{"cell_type":"markdown","source":["### Create a time duration column"],"metadata":{}},{"cell_type":"markdown","source":["# 4. Train First Model"],"metadata":{}},{"cell_type":"markdown","source":["#### Import machine learning libraries"],"metadata":{}},{"cell_type":"markdown","source":["#### Split data into training and test set"],"metadata":{}},{"cell_type":"markdown","source":["#### Convert to Vectors for machine learning"],"metadata":{}},{"cell_type":"markdown","source":["#### Initialize model"],"metadata":{}},{"cell_type":"markdown","source":["#### Construct Parameter Grid"],"metadata":{}},{"cell_type":"markdown","source":["#### Build Pipeline\n##### In the future we can turn our above data transformations into estimators and add them into the pipeline"],"metadata":{}},{"cell_type":"markdown","source":["#### Use above objects to build Cross Validator"],"metadata":{}},{"cell_type":"markdown","source":["#### Fit Cross Validator to Model\n##### This took a long time"],"metadata":{}},{"cell_type":"markdown","source":["#### Print coeffecients to determine performance"],"metadata":{}},{"cell_type":"markdown","source":["# 5. Survival Analysis\n\n#### In order to mitigate numerous activities from creating noise in the data we conducted a survival analysis on each type of event.\n#### logistic regression does not factor in time to activity. Cannot neglect the fact that ads fade from memory over time, so to compensate we conducted a survival analysis. \n#### Limiting the time scope of our analysis to two years to censor data"],"metadata":{}},{"cell_type":"code","source":["total_conversions = conversion_events_df.count()"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["#### Conduct survival analysis on each type of event"],"metadata":{}},{"cell_type":"code","source":["duration_df = final_df.groupBy('duration')\n\nagg_list = [func.count('*').alias('converters_on_day')]\n\nfor event in column_name:\n  agg_list.append(func.sum(event).alias('total_' + event))\n\n#sum the amount of emails received on each day of conversion and the number of people that converted on that day\nduration_df = duration_df.agg(*agg_list)\n\n#calculate the total number of people that have converted thus far\nsum_converted_window = Window.partitionBy().orderBy('duration')\nduration_df = duration_df.withColumn('remaining_converters', total_conversions - func.sum(duration_df.converters_on_day).over(sum_converted_window))\n\n#divide the number of events that happened by the remaining conversions\nfor event in column_name:\n  duration_df = duration_df.withColumn(event + '_hazard_probability', duration_df['total_' + event]/duration_df.remaining_converters)\nduration_df.cache()"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["### Create Dataframe of Survival Rates"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\n\n#initialize two lists, one for data, another for columns\nsurvival_rate_data = []\nduration_column = duration_df.select('duration').collect()\nsurvival_rate_data.append(Vectors.dense(duration_column))\nsurvival_rate_columns = ['duration']\n\n"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["#### Define function to get survival data from hazard data\n##### Luckily data is only length of duration of data, no more than two years, so we can use regular python"],"metadata":{}},{"cell_type":"code","source":["def calculate_survival_rate(hazard_list):\n  survival_list = []\n  for n, hazard in enumerate(hazard_list):\n    \n    if n == 0:\n      survival_val = 0\n    else:\n      survival_val = survival_list[n-1]\n    survival_list.append((1-hazard)*survival_val)\n  return survival_list"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["#### Loop thru each event type and create columns from hazard data"],"metadata":{}},{"cell_type":"code","source":["for event in column_name:\n  hazard_list = duration_df.select(event + '_hazard_probability').collect()\n  survival_rate_data.append(calculate_survival_rate(hazard_list))\n  survival_rate_columns.append(event + '_survival_probability')\n  \nsurvival_df = spark.DataFrame(survival_rate_data, survival_rate_columns)"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["#### Join data back to path dataframe on the duration of each path"],"metadata":{}},{"cell_type":"code","source":["final_weighted_df = final_df.join(survival_df, survival_df.duration = event_df.duration, 'left_outer')"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["#### Calculate survival analysis weighted touchpoint columns\n##### Multiply the survival analysis probabilty by the number of touchpoint engagements"],"metadata":{}},{"cell_type":"code","source":["for event in column_name:\n  final_weighted_df.withColumn('weighted_' + event_name, final_weighted_df[event + '_survival_probability']*final_weighted_df[event])"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["#### Create dataframe with weighted columns"],"metadata":{}},{"cell_type":"code","source":["#create list of required columns\nweighted_columns = [weighted for weighted in final_weighted_df.columns if weighted.startswith('weighted')]\n\n#add successful conversion to the end\nweighted_columns.append('successful_conversion')\n\n\nml_df = final_weighted_df.select(weighted_columns)"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["# 6. Machine Learning on survival analysis weighted data"],"metadata":{}},{"cell_type":"code","source":["drop_list = ['individual_key', 'conversion_path', 'min_date', 'max_date']\n\nml_df = final_df.select([keep for keep in final_df.columns if keep not in drop_list])"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["(trainingData, testData) = ml_df.randomSplit([0.7, 0.3], seed = 100)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["testData = testData.rdd.map(lambda x:(Vectors.dense(x[0:-1]), x[-1])).toDF([\"features\", \"label\"])\ntrainingData = trainingData.rdd.map(lambda x:(Vectors.dense(x[0:-1]), x[-1])).toDF([\"features\", \"label\"])"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["lr = LogisticRegression()"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"code","source":["paramGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"successful_conversion\")"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["# Create 5-fold CrossValidator\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=BinaryClassificationEvaluator(),\n                          numFolds=2)  "],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["wlr_cv_model = crossval.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["predictions = wlr_cv_model.transform(testData)"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":["In logistic regression, we were interested in studying how risk\nfactors were associated with presence or absence of disease.\nSometimes, though, we are interested in how a risk factor or\ntreatment affects time to disease or some other event. Or we\nmay have study dropout, and therefore subjects who we are\nnot sure if they had disease or not. In these cases, logistic\nregression is not appropriate."],"metadata":{}}],"metadata":{"name":"MultiTouchAttributionDuplicate","notebookId":425556},"nbformat":4,"nbformat_minor":0}
