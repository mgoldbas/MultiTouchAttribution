{"cells":[{"cell_type":"code","source":["%run /Users/production@aarp.com/Utils/env_variables"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%run /Users/production@aarp.com/Utils/Utils"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["tempdir = 's3a://aarp-testing/tmp/redshift_copy'"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["## Facts about this analysis\n* ### 10,000,000,000+ rows of data processed\n* ### Spark cluster with 12 r4.2xlarge instances; 732 GB of RAM, 96 CPUs"],"metadata":{}},{"cell_type":"markdown","source":["## Sidenote:\n\n* ### I like flags. \n* ### I also like window functions. \n* ### I don't like serial processing. Bottlenecks kill me. Parallelize it all"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import udf, lag, when\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql.window import Window\nfrom pyspark.sql import functions as func"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["rs_url = 'jdbc:redshift://aarp-rs-temp.c0vmann988mu.us-east-1.redshift.amazonaws.com:5439/dev?user={}&password={}'.format(rs_user, rs_password)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["# 1. Pull Data\n\n### Import the Data from Redshift"],"metadata":{}},{"cell_type":"markdown","source":["#### Set conversion event types and start time"],"metadata":{}},{"cell_type":"code","source":["conversion_event_types = (31, 26, 32, 28, 25, 27)\nstart_time = '20160101'"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### Pull all engagement events"],"metadata":{}},{"cell_type":"code","source":["engagement_events_query = \"\"\"\n\nselect individual_key, activity_type_key, date_key\nfrom individual_activity_fact\nwhere activity_type_key not in {}\nand date_key > {}\nand individual_key <> 0 \n\n\"\"\".format(conversion_event_types, start_time)\n\n#pull from redshift\nengagement_events_df = sqlContext.read.format(\"com.databricks.spark.redshift\").option('tempdir', tempdir) \\\n  .option(\"url\", rs_url) \\\n  .option(\"autoenablessl\", \"false\") \\\n  .option(\"query\",engagement_events_query) \\\n  .option(\"forward_spark_s3_credentials\",\"true\").load() "],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["#### Pull distinct engagement events\n##### We pull distinct conversion events because we want to remove the data if multiple conversion of the same type, happen with the same user on the same day. This removes a paths to conversion that are too short and will impact our analysis"],"metadata":{}},{"cell_type":"code","source":["\nconversion_events_query = \"\"\"\n\nselect distinct individual_key, activity_type_key, date_key\nfrom individual_activity_fact\nwhere activity_type_key in {}\nand date_key > {}\nand individual_key <> 0 \n\n\"\"\".format(conversion_event_types, start_time)\n\nconversion_events_df = sqlContext.read.format(\"com.databricks.spark.redshift\").option('tempdir', tempdir) \\\n  .option(\"url\", rs_url) \\\n  .option(\"autoenablessl\", \"false\") \\\n  .option(\"query\",\"select distinct individual_key, activity_type_key, date_key from individual_activity_fact\") \\\n  .option(\"forward_spark_s3_credentials\",\"true\").load() "],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["#### Pull individual dimension data"],"metadata":{}},{"cell_type":"markdown","source":["#### Union conversion and engagement data to have all event data"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import DataFrame\n\nevents_df = DataFrame.unionAll(conversion_events_df, engagement_events_df)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["# 2. Create Flags\n\n#### Create the conversion flag, to distingush conversion events"],"metadata":{}},{"cell_type":"code","source":["#create filter for conversion flag\nconversion_flag = lambda x: 1 if x in conversion_event_types else 0\nconversion_udf = udf(conversion_flag, IntegerType())\n\n#apply filter to create column\nevents_df = events_df.withColumn('conversion_flag', conversion_udf('activity_type_key'))"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### We need to identify when there is a new user as we do not want user paths to get mixed\n#### Order by the individual_key and the date, then create a column that has the previous user id, call it the last_individual_key column"],"metadata":{}},{"cell_type":"code","source":["# Window by users and dates\nlast_user_partition = Window.partitionBy().orderBy('individual_key', 'date_key')\n\n#  Apply Window object to events dataframe to get last individual key\nevents_df = events_df.withColumn('last_individual_key', func.lag(events_df.individual_key).over(last_user_partition))\nevents_df.cache()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Use the last_individual_key column to identify a new user\n#### last_individual_key column is a 1 if its a new user, 0 if its the same user"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col, expr, when\n\nnew_user_flag = expr( \"\"\"IF(individual_key = last_individual_key or last_individual_key is null, 0, 1)\"\"\")\nevents_df = events_df.withColumn('new_user_flag', new_user_flag)\nevents_df.cache()"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### Use new user flag and conversion flag to mark paths to conversion.\n\n\n#### Create a column that is the increments every time there is a new user or a conversion. This column is then a marker for new conversion paths"],"metadata":{}},{"cell_type":"code","source":["#create partioning window\nindiv_date_key_partition = Window.partitionBy().orderBy('individual_key', 'date_key')\n\n#apply sum function over individual_key and date\nevents_df = events_df.withColumn('conversion_path', func.sum(events_df.conversion_flag + events_df.new_user_flag).over(indiv_date_key_partition))\nevents_df.cache()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["### Remove last conversion from path\n#### Since the user has not converted since their last conversion flag, we want to flag the last conversion path for a user"],"metadata":{}},{"cell_type":"code","source":["conversion_rank_window = Window.partitionBy('individual_key').orderBy('individual_key', 'conversion_path')\n\nevents_df = events_df.withColumn('conversion_rank', func.rank().over(conversion_rank_window))"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["### Flag not converters\n\n#### Create a dataframe of distinct individual keys from the converter dataframe."],"metadata":{}},{"cell_type":"code","source":["converter_df = conversion_events_df.select('individual_key').groupby('individual_key').agg({'*':'count'})\n"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["converter_df = converter_df.select(col('individual_key').alias('converter'), col('count(1)').alias('num_conversions'))\n"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["#### Left outer join converter keys to events dataframe, this flags keys as converters or non-converters"],"metadata":{}},{"cell_type":"code","source":["events_df = events_df.join(converter_df, events_df.individual_key == converter_df.converter, 'left_outer')"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["## Create column to be used for model\n#### if the person has never converted, or it is their last path to conversion it is a 0, otherwise its a 1"],"metadata":{}},{"cell_type":"code","source":["model_column = when(col('converter').isNull(), 0).when(col('conversion_rank') == col('num_conversions'), 0).otherwise(1)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["#### Add model column flag to the dataframe"],"metadata":{}},{"cell_type":"code","source":["events_df = events_df.withColumn('successful_conversion', model_column)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["sample_events_df = events_df.sample(True, .0001)\nsample_events_df.createOrReplaceTempView('events_temp_table')"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["#spark.sql('select count(*) from events_temp_table').collect()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["# 3. Group dataframe around path\n\n#### Now that we have a key for conversion path, we can group engagements by that flag to give us our data"],"metadata":{}},{"cell_type":"markdown","source":["#### Widen the data with some one hot encoding"],"metadata":{}},{"cell_type":"code","source":["lambda_pairs = [(1, 'email_not_sent'),\n                (2, 'email_bounce'),\n                (3, 'email_compliant'),\n                (4, 'email_click'),\n                (5, 'email_unsubscribe'),\n                (6, 'email_open'),\n                (7, 'email_send_jobs'),\n                (8, 'email_sent'),\n                (9, 'call_center_phone'),\n                (10, 'web_click'),\n                (11, 'mobile_click'),\n                (12, 'advo_particpation'),\n                (13, 'advo_contribution'),\n                (14, 'mail_contact_dep'),\n                (15, 'ad_displayed'),\n                (16, 'driver_safety'),\n                (17, 'lifestyle_engagement'),\n                (18, 'service_participation'),\n                (19, 'center_event'),\n                (20, 'volunteer_event'),\n                (21, 'account_order'),\n                (22, 'fndn_contribution'),\n                (23, 'offer_mailed'),\n                (24, 'call_center_phone_contact'),\n                (29, 'rewards_deposited'),\n                (30, 'rewards_promo_code_used'),\n                (33, 'f_communication'),\n                (34, 'email_outbound'),\n                (35, 'mail_outbound'),\n                (36, 'phone_outbound'),\n                (37, 'text_outbound'),\n                (38, 'email_inbound'),\n                (39, 'mail_inbound'),\n                (40, 'phone_inbound'),\n                (41, 'text_inbound'),\n                (42, 'w_inbound')]"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["lambda_list = []\nfor activity_type_key, activity_name in lambda_pairs:\n  lambda_list.append((activity_name + '_flag', udf(lambda x: 1 if x == activity_type_key else 0, IntegerType())))\n"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["for column_name, udf in lambda_list:\n  events_df = events_df.withColumn(column_name,  udf('activity_type_key'))"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["event_list = [column_lambda[0] for column_lambda in lambda_list]\n"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["#sqlContext.registerDataFrameAsTable(events_df.sample(True, .00001), 'events_df_temp')"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["### Group by conversion path and individual"],"metadata":{}},{"cell_type":"code","source":["grouped_df = events_df.groupby('individual_key', 'conversion_path', 'successful_conversion')"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"markdown","source":["### Aggregate all touch points"],"metadata":{}},{"cell_type":"code","source":["#Initialize the aggregations list\naggregates = [func.max('date_key').alias('max_date'), func.max('date_key').alias('min_date')]\n\n#for each event in the event list, add a summation column\nfor event in event_list:\n  aggregates.append(func.sum(event).alias(event))\n\n#apply all aggregations to the data frame\nfinal_df = grouped_df.agg(*aggregates)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["### Create a time duration column"],"metadata":{}},{"cell_type":"code","source":["final_df = final_df.withColumn('duration', final_df.max_date - final_df.min_date)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["#drop max_date and min_date columns, they are not necessary\ndropped_columns = ['max_date', 'min_date']\ncolumn_list = [keep for keep in final_df.columns if keep not in dropped_columns]\n\n#put successful_conversion to the back, this is our label column\ncolumn_list.remove('successful_conversion')\ncolumn_list.append('successful_conversion')\nml_df = final_df.select(column_list)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["# 4. Train First Model"],"metadata":{}},{"cell_type":"markdown","source":["#### Import machine learning libraries"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":["#### Split data into training and test set"],"metadata":{}},{"cell_type":"code","source":["(trainingData, testData) = ml_df.randomSplit([0.7, 0.3], seed = 100)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":["#### Convert to Vectors for machine learning"],"metadata":{}},{"cell_type":"code","source":["testData = testData.rdd.map(lambda x:(Vectors.dense(x[0:-1]), x[-1])).toDF([\"features\", \"label\"])\ntrainingData = trainingData.rdd.map(lambda x:(Vectors.dense(x[0:-1]), x[-1])).toDF([\"features\", \"label\"])"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":["#### Initialize model"],"metadata":{}},{"cell_type":"code","source":["lr = LogisticRegression()"],"metadata":{},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":["#### Construct Parameter Grid"],"metadata":{}},{"cell_type":"code","source":["paramGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["#### Build Pipeline\n##### In the future we can turn our above data transformations into estimators and add them into the pipeline"],"metadata":{}},{"cell_type":"code","source":["pipeline = Pipeline(stages=[lr])"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":["#### Use above objects to build Cross Validator"],"metadata":{}},{"cell_type":"code","source":["# Low number of folds for testing purposes\n\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=BinaryClassificationEvaluator(),\n                          numFolds=2)  "],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":["#### Fit Cross Validator to Model\n##### This took a long time"],"metadata":{}},{"cell_type":"code","source":["lr_cv_model = crossval.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"markdown","source":["#### Print coeffecients to determine performance"],"metadata":{}},{"cell_type":"code","source":["# Print the coefficients and intercept for logistic regression\nprint(\"Coefficients: \" + str(lrModel.coefficients))\nprint(\"Intercept: \" + str(lrModel.intercept))"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":["# 5. Survival Analysis\n\n#### In order to mitigate numerous activities from creating noise in the data we conducted a survival analysis on each type of event.\n#### logistic regression does not factor in time to activity. Cannot neglect the fact that ads fade from memory over time, so to compensate we conducted a survival analysis. \n#### Limiting the time scope of our analysis to two years to censor data"],"metadata":{}},{"cell_type":"code","source":["total_conversions = conversion_events_df.count()"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":["#### Conduct survival analysis on each type of event"],"metadata":{}},{"cell_type":"code","source":["duration_df = final_df.groupBy('duration')\n\nagg_list = [func.count('*').alias('converters_on_day')]\n\nfor event in column_name:\n  agg_list.append(func.sum(event).alias('total_' + event))\n\n#sum the amount of emails received on each day of conversion and the number of people that converted on that day\nduration_df = duration_df.agg(*agg_list)\n\n#calculate the total number of people that have converted thus far\nsum_converted_window = Window.partitionBy().orderBy('duration')\nduration_df = duration_df.withColumn('remaining_converters', total_conversions - func.sum(duration_df.converters_on_day).over(sum_converted_window))\n\n#divide the number of events that happened by the remaining conversions\nfor event in column_name:\n  duration_df = duration_df.withColumn(event + '_hazard_probability', duration_df['total_' + event]/duration_df.remaining_converters)\nduration_df.cache()"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":["### Create Dataframe of Survival Rates"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\n\n#initialize two lists, one for data, another for columns\nsurvival_rate_data = []\nduration_column = duration_df.select('duration').collect()\nsurvival_rate_data.append(Vectors.dense(duration_column))\nsurvival_rate_columns = ['duration']\n\n"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":["#### Define function to get survival data from hazard data\n##### Luckily data is only length of duration of data, no more than two years, so we can use regular python"],"metadata":{}},{"cell_type":"code","source":["def calculate_survival_rate(hazard_list):\n  survival_list = []\n  for n, hazard in enumerate(hazard_list):\n    \n    if n == 0:\n      survival_val = 0\n    else:\n      survival_val = survival_list[n-1]\n    survival_list.append((1-hazard)*survival_val)\n  return survival_list"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"markdown","source":["#### Loop thru each event type and create columns from hazard data"],"metadata":{}},{"cell_type":"code","source":["for event in column_name:\n  hazard_list = duration_df.select(event + '_hazard_probability').collect()\n  survival_rate_data.append(calculate_survival_rate(hazard_list))\n  survival_rate_columns.append(event + '_survival_probability')\n  \nsurvival_df = spark.DataFrame(survival_rate_data, survival_rate_columns)"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":["#### Join data back to path dataframe on the duration of each path"],"metadata":{}},{"cell_type":"code","source":["final_weighted_df = final_df.join(survival_df, survival_df.duration = event_df.duration, 'left_outer')"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"markdown","source":["#### Calculate survival analysis weighted touchpoint columns\n##### Multiply the survival analysis probabilty by the number of touchpoint engagements"],"metadata":{}},{"cell_type":"code","source":["for event in column_name:\n  final_weighted_df.withColumn('weighted_' + event_name, final_weighted_df[event + '_survival_probability']*final_weighted_df[event])"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"markdown","source":["#### Create dataframe with weighted columns"],"metadata":{}},{"cell_type":"code","source":["#create list of required columns\nweighted_columns = [weighted for weighted in final_weighted_df.columns if weighted.startswith('weighted')]\n\n#add successful conversion to the end\nweighted_columns.append('successful_conversion')\n\n\nml_df = final_weighted_df.select(weighted_columns)"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"markdown","source":["# 6. Machine Learning on survival analysis weighted data"],"metadata":{}},{"cell_type":"code","source":["drop_list = ['individual_key', 'conversion_path', 'min_date', 'max_date']\n\nml_df = final_df.select([keep for keep in final_df.columns if keep not in drop_list])"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["(trainingData, testData) = ml_df.randomSplit([0.7, 0.3], seed = 100)"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"code","source":["testData = testData.rdd.map(lambda x:(Vectors.dense(x[0:-1]), x[-1])).toDF([\"features\", \"label\"])\ntrainingData = trainingData.rdd.map(lambda x:(Vectors.dense(x[0:-1]), x[-1])).toDF([\"features\", \"label\"])"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"code","source":["lr = LogisticRegression()"],"metadata":{},"outputs":[],"execution_count":94},{"cell_type":"code","source":["paramGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n             .addGrid(lr.maxIter, [1, 5, 10])\n             .build())"],"metadata":{},"outputs":[],"execution_count":95},{"cell_type":"code","source":["# Evaluate model\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"successful_conversion\")"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["# Create 5-fold CrossValidator\ncrossval = CrossValidator(estimator=pipeline,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=BinaryClassificationEvaluator(),\n                          numFolds=2)  "],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"code","source":["wlr_cv_model = crossval.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":98},{"cell_type":"code","source":["predictions = wlr_cv_model.transform(testData)"],"metadata":{},"outputs":[],"execution_count":99},{"cell_type":"markdown","source":["In logistic regression, we were interested in studying how risk\nfactors were associated with presence or absence of disease.\nSometimes, though, we are interested in how a risk factor or\ntreatment affects time to disease or some other event. Or we\nmay have study dropout, and therefore subjects who we are\nnot sure if they had disease or not. In these cases, logistic\nregression is not appropriate."],"metadata":{}}],"metadata":{"name":"MultiTouchAttribution","notebookId":411223},"nbformat":4,"nbformat_minor":0}
